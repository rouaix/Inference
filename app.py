"""
P2P Inference â€” Interface Gradio
Interface de gestion des modÃ¨les fragmentÃ©s, infÃ©rence et tests.
"""
# python app.py --fragments-dir models/tinyllama_q8_fragments_v2

import io
import json
import shutil
import sys
import time
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

# ============================================================
# Ã‰tat global de l'application
# ============================================================

class AppState:
    def __init__(self):
        self.engine = None          # P2PInferenceEngine chargÃ©
        self.fragments_dir = None   # RÃ©pertoire actif

state = AppState()


# ============================================================
# Utilitaires
# ============================================================

class StdoutCapture:
    """Capture sys.stdout dans un buffer."""
    def __init__(self):
        self._buf = io.StringIO()
        self._old = None

    def __enter__(self):
        self._old = sys.stdout
        sys.stdout = self
        return self

    def __exit__(self, *args):
        sys.stdout = self._old

    def write(self, text):
        self._buf.write(text)

    def flush(self):
        pass

    def getvalue(self) -> str:
        return self._buf.getvalue()


def scan_fragment_dirs(base_dir: str) -> List[Dict]:
    """Retourne la liste des rÃ©pertoires contenant un manifest.json."""
    results = []
    base = Path(base_dir) if base_dir else Path(".")
    if not base.exists():
        return results
    for item in sorted(base.iterdir()):
        if not item.is_dir():
            continue
        manifest_path = item / "manifest.json"
        if not manifest_path.exists():
            continue
        try:
            with open(manifest_path) as f:
                m = json.load(f)
            model_name = m.get("model_name", item.name)
            n_frags = m.get("total_fragments", len(m.get("fragments", [])))
            chunk_mb = m.get("chunk_size", 0) / (1024 ** 2)
            total_bytes = sum(
                fp.stat().st_size for fp in item.glob("*.dat") if fp.is_file()
            )
            results.append({
                "path": str(item),
                "name": model_name,
                "fragments": n_frags,
                "size_mb": total_bytes / (1024 ** 2),
                "chunk_mb": chunk_mb,
            })
        except Exception as e:
            results.append({
                "path": str(item),
                "name": item.name,
                "fragments": "?",
                "size_mb": 0,
                "chunk_mb": 0,
                "error": str(e),
            })
    return results


def format_dirs_table(dirs: List[Dict]) -> str:
    if not dirs:
        return "_Aucun rÃ©pertoire de fragments trouvÃ©._"
    lines = [
        "| Nom | Fragments | Taille totale | Chunk | Chemin |",
        "|-----|-----------|---------------|-------|--------|",
    ]
    for d in dirs:
        lines.append(
            f"| **{d['name']}** | {d['fragments']} | {d['size_mb']:.1f} Mo"
            f" | {d['chunk_mb']:.0f} Mo | `{d['path']}` |"
        )
    return "\n".join(lines)


# ============================================================
# Onglet 1 â€” ModÃ¨le
# ============================================================

def load_model(fragments_dir: str, verbose: bool) -> Tuple[str, str]:
    """Charge un P2PInferenceEngine depuis un rÃ©pertoire de fragments."""
    global state

    fragments_dir = fragments_dir.strip()
    if not fragments_dir or not Path(fragments_dir).exists():
        return "ERROR RÃ©pertoire invalide ou introuvable.", ""

    try:
        with StdoutCapture() as cap:
            from p2p_inference import P2PInferenceEngine
            state.engine = P2PInferenceEngine(fragments_dir, verbose=verbose)
            state.fragments_dir = fragments_dir

        cfg = state.engine.config
        info = f"""SUCCESS **ModÃ¨le chargÃ©** depuis `{fragments_dir}`

| ParamÃ¨tre | Valeur |
|-----------|--------|
| Dimensions | {cfg.dim} |
| Couches | {cfg.n_layers} |
| TÃªtes attention | {cfg.n_heads} (KV : {cfg.n_kv_heads}) |
| Vocabulaire | {cfg.vocab_size} |
| FFN dim | {cfg.hidden_dim} |
| RoPE base | {cfg.rope_freq_base} |
| Norm eps | {cfg.norm_eps} |"""
        return info, cap.getvalue()

    except Exception as e:
        tb = traceback.format_exc()
        return f"ERROR Erreur : {e}", tb


def scan_models(base_dir: str) -> str:
    dirs = scan_fragment_dirs(base_dir.strip() or ".")
    return format_dirs_table(dirs)


# ============================================================
# Onglet 2 â€” Fragmentation
# ============================================================

def run_fragmentation(gguf_path: str, output_dir: str, chunk_mb: float, progress=None):
    """Fragmente un fichier GGUF en morceaux de chunk_mb Mo."""
    gguf_path = gguf_path.strip()
    if not gguf_path or not Path(gguf_path).exists():
        yield "ERROR Fichier GGUF introuvable.", ""
        return

    output_dir = output_dir.strip()
    if not output_dir:
        output_dir = str(Path(gguf_path).parent / (Path(gguf_path).stem + "_fragments"))

    chunk_bytes = int(chunk_mb * 1024 * 1024)
    log_lines = []

    try:
        log_lines.append(f"INFO Fichier source : {gguf_path}")
        log_lines.append(f"INFO RÃ©pertoire de sortie : {output_dir}")
        log_lines.append(f"INFO Taille des chunks : {chunk_mb:.0f} Mo")
        yield "INFO Initialisation...", "\n".join(log_lines)

        from fragmenter import RealGGUFFragmenter
        frag = RealGGUFFragmenter(gguf_path, chunk_size=chunk_bytes)

        log_lines.append("INFO Lancement de la fragmentation...")
        yield "INFO Fragmentation en cours (peut prendre plusieurs minutes)...", "\n".join(log_lines)

        with StdoutCapture() as cap:
            frag.fragment(output_dir)

        log_lines.append(cap.getvalue())
        stats = frag.stats

        summary = (
            f"SUCCESS **Fragmentation terminÃ©e !**\n\n"
            f"- Fragments crÃ©Ã©s : **{stats['fragment_count']}**\n"
            f"- Volume total : **{stats['total_bytes'] / (1024**3):.3f} Go**\n"
            f"- Taille par chunk : **{chunk_mb:.0f} Mo**\n"
            f"- RÃ©pertoire : `{output_dir}`"
        )
        yield summary, "\n".join(log_lines)

    except Exception as e:
        tb = traceback.format_exc()
        log_lines.append(f"\nERROR ERREUR :\n{tb}")
        yield f"ERROR Erreur : {e}", "\n".join(log_lines)


# ============================================================
# Onglet 3 â€” Nettoyage
# ============================================================

def list_sets(base_dir: str):
    import gradio as gr
    dirs = scan_fragment_dirs(base_dir.strip() or ".")
    choices = [d["path"] for d in dirs]
    table = format_dirs_table(dirs)
    return gr.update(choices=choices, value=[]), table


def delete_selected(selected: List[str], confirmed: bool) -> str:
    if not selected:
        return "WARN Aucun rÃ©pertoire sÃ©lectionnÃ©."
    if not confirmed:
        return "WARN Cochez la case de confirmation avant de supprimer."
    msgs = []
    for path in selected:
        p = Path(path)
        if p.exists():
            shutil.rmtree(p)
            msgs.append(f"ğŸ—‘ï¸ SupprimÃ© : `{path}`")
        else:
            msgs.append(f"WARN Introuvable : `{path}`")
    return "\n\n".join(msgs)


# ============================================================
# Templates de chat par famille de modÃ¨le
# ============================================================

CHAT_TEMPLATES = {
    "TinyLlama": {
        "label": "TinyLlama 1.1B Chat",
        "template": (
            "<|system|>\nYou are a helpful assistant.</s>\n"
            "<|user|>\n{prompt}</s>\n"
            "<|assistant|>\n"
        ),
        "eos_ids": [2],
        "bos": True,
    },
    "Llama 2": {
        "label": "Llama 2 / CodeLlama",
        "template": (
            "[INST] <<SYS>>\nYou are a helpful assistant.\n<</SYS>>\n\n{prompt} [/INST]"
        ),
        "eos_ids": [2],
        "bos": True,
    },
    "Llama 3": {
        "label": "Llama 3.x (1B / 3B / 8B / 70B / 405B)",
        "template": (
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
            "You are a helpful assistant.<|eot_id|>\n"
            "<|start_header_id|>user<|end_header_id|>\n"
            "{prompt}<|eot_id|>\n"
            "<|start_header_id|>assistant<|end_header_id|>\n"
        ),
        "eos_ids": [128009, 128001],  # <|eot_id|> et <|end_of_text|>
        "bos": False,  # dÃ©jÃ  dans le template
    },
    "Mistral": {
        "label": "Mistral / Nemo / Large",
        "template": "[INST] {prompt} [/INST]",
        "eos_ids": [2],
        "bos": True,
    },
}

CHAT_FAMILY_CHOICES = [v["label"] for v in CHAT_TEMPLATES.values()]
_LABEL_TO_KEY = {v["label"]: k for k, v in CHAT_TEMPLATES.items()}


# ============================================================
# Onglet 4 â€” Chat / InfÃ©rence
# ============================================================


def run_chat(
    prompt: str,
    history: list,
    max_tokens: int,
    temperature: float,
    top_k: int,
    top_p: float,
    verbose: bool,
    model_family: str,
):
    """GÃ©nÃ¨re une rÃ©ponse token par token (streaming via yield).
    Utilise le prefill complet : toute la sÃ©quence est passÃ©e Ã  chaque step.
    """
    global state

    if state.engine is None:
        new_hist = history + [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": "ERROR Aucun modÃ¨le chargÃ©. Allez dans l'onglet **ModÃ¨le**."},
        ]
        yield new_hist, ""
        return

    if not prompt.strip():
        yield history, ""
        return

    from p2p_inference import LlamaLayer, rms_norm, _sample_logits

    engine = state.engine
    engine.verbose = verbose

    # SÃ©lection du template selon la famille de modÃ¨le
    family_key = _LABEL_TO_KEY.get(model_family, "TinyLlama")
    tpl_cfg = CHAT_TEMPLATES[family_key]
    formatted_prompt = tpl_cfg["template"].format(prompt=prompt)

    # Encodage du prompt
    tokens = engine.tokenizer.encode(formatted_prompt)
    if tpl_cfg["bos"] and (not tokens or tokens[0] != 1):
        tokens = [1] + tokens

    new_hist = history + [{"role": "user", "content": prompt}]

    # Chargement des poids fixes (une seule fois)
    w_emb = engine.load_tensor("token_embd.weight")
    if w_emb.ndim == 2 and w_emb.shape[0] == engine.config.dim and w_emb.shape[1] == engine.config.vocab_size:
        w_emb = w_emb.T  # â†’ [vocab, dim]

    w_out = engine.load_tensor("output.weight")
    w_norm = engine.load_tensor("output_norm.weight")
    if w_norm.shape != (engine.config.dim,):
        w_norm_alt = engine.load_tensor("norm.weight")
        if w_norm_alt.shape == (engine.config.dim,):
            w_norm = w_norm_alt

    generated_tokens: List[int] = []
    generated_text = ""
    log_lines = [
        f"Famille : {model_family} | Prompt : {len(tokens)} token(s) | "
        f"Max : {max_tokens} | T={temperature} K={top_k} P={top_p}"
    ]
    # EOS : liste de tokens d'arrÃªt selon la famille (ex. Llama 3 en a deux)
    eos_ids = set(tpl_cfg["eos_ids"])
    # Ajouter l'eos du tokenizer si disponible
    tok_eos = getattr(engine.tokenizer, "eos_id", 2)
    eos_ids.add(tok_eos)

    for i in range(max_tokens):
        t0 = time.time()

        # Fix prefill : embed TOUTE la sÃ©quence (prompt + tokens gÃ©nÃ©rÃ©s)
        all_tokens = tokens + generated_tokens
        valid = [t for t in all_tokens if 0 <= t < w_emb.shape[0]]
        x = w_emb[valid]  # [seq_len, dim]

        for l in range(engine.config.n_layers):
            layer = LlamaLayer(engine, l)
            x, _, _ = layer.forward(x, engine.freqs_cis, None, None, start_pos=0)

        # PrÃ©diction sur le dernier token de la sÃ©quence
        x_last = rms_norm(x[-1:], w_norm, engine.config.norm_eps)
        logits = (x_last @ w_out).flatten()

        next_token = _sample_logits(logits, temperature, top_k, top_p)

        if next_token in eos_ids:
            log_lines.append(f"Token {i+1}: <EOS> (id={next_token}) â€” arrÃªt.")
            break

        generated_tokens.append(next_token)
        generated_text = engine.tokenizer.decode(generated_tokens)

        dt = time.time() - t0
        word = engine.tokenizer.decode([next_token])
        log_lines.append(f"Token {i+1}: '{word}' (id={next_token}) en {dt:.2f}s")

        # Mise Ã  jour streaming
        stream_hist = new_hist + [{"role": "assistant", "content": generated_text + "â–Œ"}]
        yield stream_hist, "\n".join(log_lines)

    final_hist = new_hist + [{"role": "assistant", "content": generated_text or "_(rÃ©ponse vide)_"}]
    yield final_hist, "\n".join(log_lines)


# ============================================================
# Onglet 5 â€” Tests
# ============================================================

def run_system_tests() -> str:
    """VÃ©rifie que tous les composants sont fonctionnels."""
    global state
    results = []
    ok = True

    def check(label, fn):
        nonlocal ok
        try:
            fn()
            results.append(f"SUCCESS {label}")
        except Exception as e:
            results.append(f"ERROR {label} â€” {e}")
            ok = False

    def warn(label, fn):
        try:
            fn()
            results.append(f"SUCCESS {label}")
        except Exception as e:
            results.append(f"WARN {label} â€” {e} _(optionnel)_")

    check("Python & numpy", lambda: np.zeros(1))

    warn("Module `gguf`", lambda: __import__("gguf"))
    warn("Module `sentencepiece`", lambda: __import__("sentencepiece"))

    import gradio
    check(f"Gradio {gradio.__version__}", lambda: None)

    check("p2p_inference importable", lambda: __import__("p2p_inference"))
    check("fragmenter importable", lambda: __import__("fragmenter"))
    check("recombiner importable", lambda: __import__("recombiner"))

    def test_rope():
        from p2p_inference import precompute_freqs_cis, apply_rotary_emb
        freqs = precompute_freqs_cis(64, 128)
        assert freqs.shape == (128, 32), f"Shape inattendue : {freqs.shape}"
        xq = np.random.randn(4, 8, 64).astype(np.float32)
        xk = np.random.randn(4, 8, 64).astype(np.float32)
        xq_r, xk_r = apply_rotary_emb(xq, xk, freqs)
        assert xq_r.shape == xq.shape

    check("RoPE (precompute + apply)", test_rope)

    def test_softmax():
        from p2p_inference import softmax
        x = np.array([[1.0, 2.0, 3.0]])
        s = softmax(x)
        assert abs(s.sum() - 1.0) < 1e-5

    check("Softmax", test_softmax)

    def test_rms_norm():
        from p2p_inference import rms_norm
        x = np.ones((1, 64), dtype=np.float32)
        w = np.ones(64, dtype=np.float32)
        out = rms_norm(x, w, 1e-5)
        assert out.shape == x.shape

    check("RMS Norm", test_rms_norm)

    if state.engine is not None:
        cfg = state.engine.config
        check(f"ModÃ¨le chargÃ© ({cfg.n_layers}L dim={cfg.dim})", lambda: None)

        def test_tokenizer():
            tok = state.engine.tokenizer
            ids = tok.encode("Hello world")
            decoded = tok.decode(ids)
            results.append(f"    â†’ 'Hello world' â†’ ids={ids[:5]}â€¦ â†’ '{decoded[:30]}'")

        check("Tokenizer encode/decode", test_tokenizer)

        def test_embedding():
            w = state.engine.load_tensor("token_embd.weight")
            assert w.ndim == 2, f"Embedding doit Ãªtre 2D, obtenu {w.ndim}D"
            results.append(f"    â†’ shape={w.shape} dtype={w.dtype}")

        check("Chargement embedding", test_embedding)
    else:
        results.append("â­ï¸ Tests modÃ¨le : skipped (aucun modÃ¨le chargÃ©)")

    banner = "SUCCESS **Tous les tests rÃ©ussis !**" if ok else "WARN **Certains tests ont Ã©chouÃ©.**"
    return banner + "\n\n" + "\n\n".join(results)


def run_quality_test(custom_prompt: str) -> str:
    """GÃ©nÃ¨re quelques tokens sur des prompts de rÃ©fÃ©rence pour Ã©valuer la qualitÃ©."""
    global state

    if state.engine is None:
        return "ERROR Aucun modÃ¨le chargÃ©."

    from p2p_inference import LlamaLayer, rms_norm

    engine = state.engine
    test_cases = [
        ("Hello, my name is", "Texte libre"),
        ("1 + 1 =", "ArithmÃ©tique"),
        ("The capital of France is", "Connaissance"),
    ]
    if custom_prompt.strip():
        test_cases.append((custom_prompt.strip(), "PersonnalisÃ©"))

    w_emb = engine.load_tensor("token_embd.weight")
    if w_emb.ndim == 2 and w_emb.shape[0] == engine.config.dim and w_emb.shape[1] == engine.config.vocab_size:
        w_emb = w_emb.T

    w_out = engine.load_tensor("output.weight")
    w_norm = engine.load_tensor("output_norm.weight")
    if w_norm.shape != (engine.config.dim,):
        w_norm_alt = engine.load_tensor("norm.weight")
        if w_norm_alt.shape == (engine.config.dim,):
            w_norm = w_norm_alt

    # Nombre de couches limitÃ© pour la vitesse des tests de qualitÃ©
    n_layers_test = min(engine.config.n_layers, 4)
    eos_id = getattr(engine.tokenizer, "eos_id", 2)

    output_parts = []
    for prompt, label in test_cases:
        try:
            tokens = engine.tokenizer.encode(prompt)
            if not tokens or tokens[0] != 1:
                tokens = [1] + tokens

            generated = []

            for _ in range(15):
                # Prefill : toute la sÃ©quence
                all_tokens = tokens + generated
                valid = [t for t in all_tokens if 0 <= t < w_emb.shape[0]]
                x = w_emb[valid]
                for l in range(n_layers_test):
                    layer = LlamaLayer(engine, l)
                    x, _, _ = layer.forward(x, engine.freqs_cis, None, None, 0)
                x_last = rms_norm(x[-1:], w_norm, engine.config.norm_eps)
                logits = (x_last @ w_out).flatten()
                next_token = int(np.argmax(logits))
                if next_token == eos_id:
                    break
                generated.append(next_token)

            text = engine.tokenizer.decode(generated)
            output_parts.append(f"**{label}** â€” `{prompt}` â†’ `{text}`")
        except Exception as e:
            output_parts.append(f"**{label}** â€” `{prompt}` â†’ ERROR {e}")

    return "\n\n".join(output_parts)


# ============================================================
# Onglet 6 â€” ParamÃ¨tres
# ============================================================

def apply_params(
    max_tokens: int,
    temperature: float,
    top_k: int,
    top_p: float,
    verbose: bool,
    rope_base: float,
    norm_eps: float,
) -> str:
    global state

    msgs = ["SUCCESS ParamÃ¨tres enregistrÃ©s."]

    if state.engine is not None:
        cfg = state.engine.config
        cfg.rope_freq_base = float(rope_base)
        cfg.norm_eps = float(norm_eps)
        state.engine.verbose = verbose

        # Recalcul RoPE
        from p2p_inference import precompute_freqs_cis
        state.engine.freqs_cis = precompute_freqs_cis(
            cfg.dim // cfg.n_heads,
            cfg.dim * 2,
            theta=float(rope_base),
        )
        msgs.append(f"ğŸ”„ RoPE recalculÃ© (base={rope_base:.0f}).")
        msgs.append(f"ğŸ”§ ModÃ¨le mis Ã  jour : {cfg.n_layers}L dim={cfg.dim}.")
    else:
        msgs.append("_(Aucun modÃ¨le chargÃ© â€” les valeurs seront appliquÃ©es au prochain chargement)_")

    return "\n\n".join(msgs)


# ============================================================
# Construction de l'interface Gradio
# ============================================================

def build_app():
    import gradio as gr

    with gr.Blocks(title="P2P Inference UI") as demo:

        gr.Markdown("# ğŸŒ P2P Inference â€” Interface de gestion")
        gr.Markdown(
            "Chargez, fragmentez et interrogez des modÃ¨les IA distribuÃ©s en peer-to-peer."
        )

        with gr.Tabs():

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ONGLET 1 : MODÃˆLE
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            with gr.Tab("MODELE ModÃ¨le"):
                gr.Markdown("## Charger un modÃ¨le fragmentÃ©")

                with gr.Row():
                    with gr.Column(scale=3):
                        frag_dir_box = gr.Textbox(
                            label="RÃ©pertoire des fragments",
                            placeholder="Ex : ./tinyllama_fragments",
                            info="RÃ©pertoire contenant manifest.json et les .dat",
                        )
                        verbose_cb = gr.Checkbox(label="Verbose", value=False)
                        load_btn = gr.Button("ğŸš€ Charger le modÃ¨le", variant="primary")

                    with gr.Column(scale=2):
                        gr.Markdown("**ModÃ¨les disponibles**")
                        scan_dir_box = gr.Textbox(label="Dossier Ã  scanner", value=".", placeholder=".")
                        scan_btn = gr.Button("ğŸ” Scanner", size="sm")
                        scan_result = gr.Markdown("_Cliquez sur Scanner_")

                model_info = gr.Markdown("_Aucun modÃ¨le chargÃ©_")
                load_log_box = gr.Textbox(
                    label="Logs", lines=8, interactive=False, elem_classes="mono"
                )

                load_btn.click(load_model, [frag_dir_box, verbose_cb], [model_info, load_log_box])
                scan_btn.click(scan_models, [scan_dir_box], [scan_result])

                gr.Markdown("""
---
## ModÃ¨les compatibles avec le moteur Python

Tous ces modÃ¨les sont de **famille LLaMA** (architecture identique : RoPE, RMSNorm, SwiGLU, GQA).
Les paramÃ¨tres du modÃ¨le (couches, dimensions, RoPEâ€¦) sont lus automatiquement depuis le GGUF.
Seul le **chat template** dans l'onglet Chat doit Ãªtre adaptÃ© selon le modÃ¨le chargÃ©.

### Petits modÃ¨les (< 5 Go)

| ModÃ¨le | Taille Q4_K_M | Ajustements requis |
|--------|--------------|-------------------|
| **TinyLlama 1.1B Chat** _(actuel)_ | ~0.7 Go | Aucun â€” dÃ©jÃ  configurÃ© |
| **Llama 3.2 1B Instruct** | ~0.8 Go | Chat template Llama 3 + RoPE base = 500 000 |
| **Llama 3.2 3B Instruct** | ~2 Go | Chat template Llama 3 + RoPE base = 500 000 |
| **Mistral 7B Instruct v0.3** | ~4.1 Go | Chat template `[INST]...[/INST]` |

### ModÃ¨les moyens (5â€“50 Go)

| ModÃ¨le | Taille Q4_K_M | Ajustements requis |
|--------|--------------|-------------------|
| **Llama 3.1 8B Instruct** | ~4.7 Go | Chat template Llama 3 + RoPE base = 500 000 + vocab = 128 256 |
| **CodeLlama 13B Instruct** | ~7.4 Go | Chat template code + RoPE base = 1 000 000 |
| **Mistral Nemo 12B** | ~7 Go | Chat template Mistral + Norm eps = 1e-6 |
| **Llama 3.1 70B Instruct** | ~40 Go | Chat template Llama 3 + RoPE base = 500 000 â€” RAM : ~48 Go |

### Grands modÃ¨les (> 50 Go) â€” cibles P2P

| ModÃ¨le | Taille Q4_K_M | Ajustements requis |
|--------|--------------|-------------------|
| **Mistral Large 2 123B** | ~70 Go | Chat template Mistral |
| **Llama 3.1 405B** | ~229 Go | Chat template Llama 3 + RoPE base = 500 000 |
| **Mistral Large 3 675B** _(cible)_ | ~338 Go | Chat template Mistral |

### Chat templates de rÃ©fÃ©rence

**TinyLlama** _(actuel)_
```
<|system|>
You are a helpful assistant.</s>
<|user|>
{prompt}</s>
<|assistant|>
```

**Llama 3.x**
```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a helpful assistant.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
{prompt}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
```

**Mistral / Nemo / Large**
```
[INST] {prompt} [/INST]
```

> **ModÃ¨les non compatibles avec le moteur Python** (bridge uniquement) :
> Phi-3, Gemma, Qwen, Mixtral 8Ã—7B â€” ces architectures ne sont pas LLaMA et nÃ©cessiteraient une rÃ©Ã©criture du moteur.
""")


            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ONGLET 2 : FRAGMENTATION
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            with gr.Tab("âœ‚ï¸ Fragmentation"):
                gr.Markdown("## Fragmenter un fichier GGUF")

                with gr.Row():
                    with gr.Column():
                        gguf_box = gr.Textbox(
                            label="Fichier GGUF source",
                            placeholder="Ex : C:/models/tinyllama.gguf",
                        )
                        out_dir_box = gr.Textbox(
                            label="RÃ©pertoire de sortie",
                            placeholder="Laissez vide â†’ mÃªme dossier que le .gguf",
                        )
                        chunk_slider = gr.Slider(
                            1, 100, value=10, step=1,
                            label="Taille des fragments (Mo)",
                            info="10 Mo = recommandÃ© pour P2P",
                        )
                        frag_btn = gr.Button("âš¡ Lancer la fragmentation", variant="primary")

                    with gr.Column():
                        gr.Markdown("""
**Formats GGUF supportÃ©s**

| Format | Bits/poids | QualitÃ© |
|--------|-----------|---------|
| Q4_K_M | 4.5 | â˜…â˜…â˜…â˜…â˜† |
| Q8_0   | 8   | â˜…â˜…â˜…â˜…â˜† |
| F16    | 16  | â˜…â˜…â˜…â˜…â˜… |
| F32    | 32  | â˜…â˜…â˜…â˜…â˜… |

Le fichier `manifest.json` gÃ©nÃ©rÃ© indexe tous les fragments
et permet la reconstruction ou l'infÃ©rence distribuÃ©e.
""")

                frag_status = gr.Markdown("_PrÃªt_")
                frag_log_box = gr.Textbox(
                    label="Logs de fragmentation", lines=12,
                    interactive=False, elem_classes="mono"
                )

                frag_btn.click(
                    run_fragmentation,
                    [gguf_box, out_dir_box, chunk_slider],
                    [frag_status, frag_log_box],
                )

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ONGLET 3 : NETTOYAGE
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            with gr.Tab("ğŸ—‘ï¸ Nettoyage"):
                gr.Markdown("## Supprimer des modÃ¨les et leurs fragments")

                with gr.Row():
                    clean_base_box = gr.Textbox(label="Dossier Ã  scanner", value=".", placeholder=".")
                    list_btn = gr.Button("ğŸ” Lister", variant="secondary")

                clean_table = gr.Markdown("_Cliquez sur Lister_")
                dirs_check = gr.CheckboxGroup(
                    choices=[],
                    label="RÃ©pertoires Ã  supprimer",
                    info="Cochez les rÃ©pertoires de fragments Ã  effacer dÃ©finitivement",
                )

                with gr.Row():
                    confirm_cb = gr.Checkbox(
                        label="WARN Je confirme la suppression dÃ©finitive (irrÃ©versible)",
                        value=False,
                    )
                    delete_btn = gr.Button("ğŸ—‘ï¸ Supprimer la sÃ©lection", variant="stop")

                delete_result = gr.Markdown()

                list_btn.click(list_sets, [clean_base_box], [dirs_check, clean_table])
                delete_btn.click(delete_selected, [dirs_check, confirm_cb], [delete_result])

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ONGLET 4 : CHAT
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            with gr.Tab("ğŸ’¬ Chat"):
                gr.Markdown("## Dialogue avec le modÃ¨le")

                with gr.Row():
                    with gr.Column(scale=3):
                        chatbot = gr.Chatbot(
                            label="Conversation",
                            height=460,
                        )
                        with gr.Row():
                            prompt_box = gr.Textbox(
                                placeholder="Votre messageâ€¦",
                                scale=5,
                                show_label=False,
                                container=False,
                            )
                            send_btn = gr.Button("â¤ Envoyer", variant="primary", scale=1)
                        with gr.Row():
                            clear_btn = gr.Button("ğŸ—‘ï¸ Effacer", size="sm")
                            stop_btn = gr.Button("â¹ï¸ ArrÃªter", size="sm", variant="stop")

                    with gr.Column(scale=1):
                        gr.Markdown("**Famille de modÃ¨le**")
                        chat_family = gr.Dropdown(
                            choices=CHAT_FAMILY_CHOICES,
                            value=CHAT_FAMILY_CHOICES[0],
                            label="Template de chat",
                            info="Doit correspondre au modÃ¨le chargÃ©",
                        )
                        gr.Markdown("**ParamÃ¨tres de gÃ©nÃ©ration**")
                        chat_max_tokens = gr.Slider(1, 500, value=50, step=1, label="Max tokens")
                        chat_temp = gr.Slider(0.0, 2.0, value=1.0, step=0.05, label="TempÃ©rature")
                        chat_topk = gr.Slider(0, 200, value=40, step=1, label="Top-K")
                        chat_topp = gr.Slider(0.1, 1.0, value=0.95, step=0.01, label="Top-P")
                        chat_verbose = gr.Checkbox(label="Verbose logs", value=False)
                        gr.Markdown("""
---
**RÃ©glages typiques**

| Usage | Temp | Top-K | Top-P |
|-------|------|-------|-------|
| Factuel | 0.3â€“0.5 | 20â€“40 | 0.90 |
| Conversation | 0.7â€“1.0 | 40 | 0.95 |
| CrÃ©atif | 1.0â€“1.5 | 100+ | 0.98 |
| DÃ©terministe | 0.0 | 1 | â€” |
""")

                inference_log_box = gr.Textbox(
                    label="Logs d'infÃ©rence", lines=5,
                    interactive=False, elem_classes="mono"
                )

                gen_inputs = [prompt_box, chatbot, chat_max_tokens, chat_temp, chat_topk, chat_topp, chat_verbose, chat_family]
                gen_outputs = [chatbot, inference_log_box]

                gen_event = send_btn.click(run_chat, gen_inputs, gen_outputs)
                prompt_box.submit(run_chat, gen_inputs, gen_outputs)
                clear_btn.click(lambda: ([], ""), outputs=[chatbot, inference_log_box])
                stop_btn.click(fn=None, cancels=[gen_event])

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ONGLET 5 : TESTS
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            with gr.Tab("ğŸ§ª Tests"):
                gr.Markdown("## VÃ©rification du bon fonctionnement")

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### Tests systÃ¨me")
                        gr.Markdown(
                            "VÃ©rifie les imports, RoPE, tokenizer, chargement de fragments, etc."
                        )
                        sys_test_btn = gr.Button("â–¶ï¸ Lancer les tests systÃ¨me", variant="primary")
                        sys_test_out = gr.Markdown("_Cliquez pour lancer_")

                    with gr.Column():
                        gr.Markdown("### Tests de qualitÃ©")
                        gr.Markdown(
                            "GÃ©nÃ¨re quelques tokens sur des prompts de rÃ©fÃ©rence. "
                            "Requiert un modÃ¨le chargÃ©."
                        )
                        quality_prompt_box = gr.Textbox(
                            label="Prompt personnalisÃ© (optionnel)",
                            placeholder="Ex : What is machine learning?",
                        )
                        quality_btn = gr.Button("â–¶ï¸ Tester la qualitÃ©", variant="secondary")
                        quality_out = gr.Markdown("_Chargez un modÃ¨le puis cliquez_")

                sys_test_btn.click(run_system_tests, outputs=[sys_test_out])
                quality_btn.click(run_quality_test, [quality_prompt_box], [quality_out])

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ONGLET 6 : PARAMÃˆTRES
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            with gr.Tab("PARAMETRES ParamÃ¨tres"):
                gr.Markdown("## Ajustement des variables d'infÃ©rence")

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### GÃ©nÃ©ration par dÃ©faut")
                        p_max_tokens = gr.Slider(1, 2000, value=50, step=1, label="Max tokens")
                        p_temp = gr.Slider(0.0, 3.0, value=1.0, step=0.05, label="TempÃ©rature (0 = greedy)")
                        p_topk = gr.Slider(0, 500, value=40, step=1, label="Top-K (0 = dÃ©sactivÃ©)")
                        p_topp = gr.Slider(0.1, 1.0, value=0.95, step=0.01, label="Top-P (nucleus)")
                        p_verbose = gr.Checkbox(label="Verbose (afficher les fragments chargÃ©s)", value=False)

                    with gr.Column():
                        gr.Markdown("### Configuration du modÃ¨le _(avancÃ©)_")
                        gr.Markdown(
                            "âš ï¸ Modifie la configuration du modÃ¨le actuellement chargÃ©. "
                            "Ces valeurs sont normalement lues automatiquement depuis le GGUF. "
                            "Ne les modifier que si le chargement produit des rÃ©sultats incorrects."
                        )
                        p_rope_base = gr.Number(
                            value=10000.0,
                            label="RoPE freq base",
                            info="Base de frÃ©quence des embeddings positionnels rotatifs (dÃ©faut : 10000)",
                        )
                        gr.Markdown("""
**RoPE (Rotary Position Embedding)** encode la position de chaque token dans la sÃ©quence en faisant
"tourner" les vecteurs d'attention. La base contrÃ´le la vitesse de rotation :
- `10000` â†’ LLaMA 1 / TinyLlama (contexte ~2 048 tokens)
- `500000` â†’ Llama 3 (contexte long)

Plus la base est Ã©levÃ©e, mieux le modÃ¨le gÃ¨re les longues sÃ©quences.
""")
                        p_norm_eps = gr.Number(
                            value=1e-5,
                            label="Norm epsilon",
                            info="Epsilon pour RMSNorm (dÃ©faut : 1e-5)",
                        )
                        gr.Markdown("""
**RMSNorm** normalise les activations entre chaque couche via `x / sqrt(mean(xÂ²) + Îµ)`.
L'epsilon Ã©vite une division par zÃ©ro quand les activations sont proches de 0.
- `1e-5` â†’ LLaMA / TinyLlama
- `1e-6` â†’ Mistral

Modifier cette valeur sans connaÃ®tre celle du modÃ¨le dÃ©grade les sorties.
""")
                        gr.Markdown("""
### RÃ©fÃ©rence des quantifications

La quantification rÃ©duit la prÃ©cision des poids du modÃ¨le pour Ã©conomiser de la mÃ©moire et accÃ©lÃ©rer l'infÃ©rence sur CPU.

| Format | Bits/poids | QualitÃ© | Vitesse CPU |
|--------|-----------|---------|-------------|
| F32    | 32        | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | â–‘â–‘â–‘â–‘â–‘       |
| F16    | 16        | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  | â–ˆâ–ˆâ–‘â–‘â–‘       |
| Q8_0   | 8         | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  | â–ˆâ–ˆâ–ˆâ–‘â–‘       |
| Q6_K   | 6.5       | â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  | â–ˆâ–ˆâ–ˆâ–ˆâ–‘       |
| Q4_K_M | 4.5      | â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       |
| Q2_K   | 2.6       | â–ˆâ–ˆâ–‘â–‘â–‘â–‘  | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       |

**Pourquoi les formats compressÃ©s sont plus rapides ?**
Le goulot d'Ã©tranglement sur CPU est la bande passante mÃ©moire, pas le calcul.
Q4 lit 8Ã— moins de donnÃ©es que F32 â†’ 8Ã— moins de lectures RAM.

**Taille pour Mistral Large 3 (675B)**

| Format | Taille disque | RAM requise |
|--------|--------------|-------------|
| F32    | ~2 700 Go    | ~2 700 Go   |
| Q8_0   | ~675 Go      | ~675 Go     |
| Q4_K_M | ~338 Go     | ~338 Go     |
| Q2_K   | ~175 Go      | ~175 Go     |

En P2P avec Q4_K_M, chaque nÅ“ud stockant 10 Mo contribue Ã  porter collectivement les 338 Go.
""")

                apply_btn = gr.Button("APPLIQUER Appliquer", variant="primary")
                params_out = gr.Markdown()

                apply_btn.click(
                    apply_params,
                    [p_max_tokens, p_temp, p_topk, p_topp, p_verbose, p_rope_base, p_norm_eps],
                    [params_out],
                )

    return demo


# ============================================================
# Point d'entrÃ©e
# ============================================================

if __name__ == "__main__":
    import argparse
    import gradio as gr

    parser = argparse.ArgumentParser(description="P2P Inference â€” Interface Gradio")
    parser.add_argument("--host", default="127.0.0.1", help="Adresse d'Ã©coute (dÃ©faut : 127.0.0.1)")
    parser.add_argument("--port", type=int, default=7860, help="Port (dÃ©faut : 7860)")
    parser.add_argument("--share", action="store_true", help="CrÃ©er un lien public Gradio")
    parser.add_argument(
        "--fragments-dir",
        default=None,
        help="Charger automatiquement ce rÃ©pertoire de fragments au dÃ©marrage",
    )
    args = parser.parse_args()

    # Chargement automatique si spÃ©cifiÃ©
    if args.fragments_dir:
        print(f"Chargement automatique : {args.fragments_dir}")
        info, log = load_model(args.fragments_dir, verbose=False)
        print(info)
        if log:
            print(log)

    demo = build_app()
    demo.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        show_error=True,
        theme=gr.themes.Soft(),
        css=".mono textarea { font-family: 'Courier New', monospace; font-size: 12px; }",
    )
