# ModÃ¨les

> **Test en cours :** `Magistral-Small-2509-Q4_K_M.gguf`

Ce dossier contient les modÃ¨les GGUF et leurs fragments pour l'infÃ©rence P2P.

---

## ğŸ“ Contenu

### ModÃ¨les GGUF
- **tinyllama-1.1b-chat-v1.0.Q8_0.gguf** (1.09 GB) - ModÃ¨le principal quantifiÃ© Q8_0
- **tinyllama.gguf** (638 MB) - ModÃ¨le TinyLlama original

### Fragments P2P
- **tinyllama_q8_fragments_v2/** - Fragments du modÃ¨le Q8_0 (279 fragments)
  - `manifest.json` - MÃ©tadonnÃ©es et index des fragments
  - `gguf_header.dat` - En-tÃªte GGUF
  - `fragment_*.dat` - Fragments de tenseurs

### Tokenizer
- **tokenizer.model** - Tokenizer SentencePiece pour TinyLlama

---

## ğŸš€ Utilisation

### Avec le Pont Hybride (RecommandÃ©) âœ…
```python
from p2p_bridge import P2PBridge

bridge = P2PBridge("models/tinyllama_q8_fragments_v2")
text = bridge.generate("Hello", max_tokens=50)
print(text)
```

### Avec le Moteur Python Pur
```bash
python p2p_inference.py models/tinyllama_q8_fragments_v2 --prompt "Hello" --max-tokens 10
```

### Fragmenter un Nouveau ModÃ¨le
```bash
python fragmenter.py models/nouveau_modele.gguf models/nouveau_modele_fragments
```

---

## ğŸ“Š Informations sur TinyLlama

**ModÃ¨le**: TinyLlama-1.1B-Chat-v1.0
**Architecture**: Llama 2
**Taille**: 1.1 milliard de paramÃ¨tres
**Quantification**: Q8_0 (8-bit)
**Contexte**: 2048 tokens
**Vocabulaire**: 32,000 tokens

**Configuration**:
- Dimension: 2048
- TÃªtes d'attention: 32
- TÃªtes KV (GQA): 4
- Couches: 22
- FFN dimension: 5632
- RoPE theta: 10000.0

---

## ğŸ”§ Fragmentation

Le modÃ¨le Q8_0 a Ã©tÃ© fragmentÃ© en 279 fragments pour permettre le chargement progressif:

```
Total size: 1.09 GB
Fragment size: ~4 MB each
Fragments: 279
Format: Q8_0 (quantifiÃ© 8-bit)
```

**Avantages de la fragmentation**:
- âœ… Chargement progressif des couches
- âœ… RÃ©duction de l'empreinte mÃ©moire
- âœ… Distribution P2P facilitÃ©e
- âœ… Reconstruction lossless garantie

---

## ğŸ“ Notes

- Le modÃ¨le Q8_0 est utilisÃ© pour tous les tests et la production
- Les fragments v2 incluent l'en-tÃªte GGUF pour reconstruction complÃ¨te
- Le tokenizer SentencePiece est requis pour l'encodage/dÃ©codage

---

## ğŸ¯ Recommandation

Pour la production, utilisez le **pont hybride** (`p2p_bridge.py`) qui combine:
- Fragmentation P2P pour la distribution
- llama.cpp pour l'infÃ©rence (performance optimale)
- GÃ©nÃ©ration de texte cohÃ©rent garantie
