#!/usr/bin/env python3
"""
Script de dÃ©ploiement en production.
PrÃ©pare le systÃ¨me d'infÃ©rence distribuÃ©e avec les modÃ¨les fragmentÃ©s.
"""

import sys
import subprocess
from pathlib import Path
import json
import shutil

def run_command(cmd, description):
    """ExÃ©cute une commande et affiche le rÃ©sultat."""
    print(f"\nğŸ”§ {description}")
    print(f"   Commande: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print(f"   âœ… SuccÃ¨s")
            return True
        else:
            print(f"   âŒ Ã‰chec")
            if result.stderr:
                print(f"   Erreur: {result.stderr[:200]}")
            return False
    except subprocess.TimeoutExpired:
        print(f"   âŒ Timeout")
        return False
    except Exception as e:
        print(f"   âŒ Exception: {e}")
        return False

def verify_model_fragments(model_dir):
    """VÃ©rifie qu'un modÃ¨le est correctement fragmentÃ©."""
    print(f"\nğŸ” VÃ©rification du modÃ¨le: {model_dir.name}")
    
    # VÃ©rifier le manifest
    manifest_path = model_dir / "manifest.json"
    if not manifest_path.exists():
        print(f"   âŒ Manifest manquant")
        return False
    
    # Charger le manifest
    try:
        with open(manifest_path, 'r') as f:
            manifest = json.load(f)
        
        fragments_count = len(manifest.get('fragments', []))
        total_fragments = manifest.get('total_fragments', 0)
        
        print(f"   âœ… Manifest valide")
        print(f"   Fragments dÃ©clarÃ©s: {total_fragments}")
        print(f"   Fragments dans le manifest: {fragments_count}")
        
        # VÃ©rifier les fichiers de fragments
        fragment_files = list(model_dir.glob("*.dat"))
        print(f"   Fichiers .dat trouvÃ©s: {len(fragment_files)}")
        
        # VÃ©rifier le tokenizer
        tokenizer_path = model_dir / "tokenizer.json"
        if tokenizer_path.exists():
            print(f"   âœ… Tokenizer prÃ©sent")
        else:
            print(f"   âš ï¸ Tokenizer manquant")
        
        return fragments_count > 0 and len(fragment_files) > 0
        
    except Exception as e:
        print(f"   âŒ Erreur de lecture du manifest: {e}")
        return False

def prepare_deployment():
    """PrÃ©pare le dÃ©ploiement en production."""
    print("Preparation du dÃ©ploiement en production")
    print("=" * 60)
    
    # 1. VÃ©rifier l'environnement
    print("\nğŸ“‹ VÃ©rification de l'environnement...")
    
    # VÃ©rifier Python
    python_cmd = [sys.executable, "--version"]
    result = subprocess.run(python_cmd, capture_output=True, text=True)
    print(f"   Python: {result.stdout.strip()}")
    
    # VÃ©rifier les dÃ©pendances
    dependencies = ["numpy", "gguf", "requests", "zstandard"]
    for dep in dependencies:
        try:
            __import__(dep)
            print(f"   âœ… {dep} installÃ©")
        except ImportError:
            print(f"   âŒ {dep} manquant")
    
    # 2. VÃ©rifier les modÃ¨les fragmentÃ©s
    print(f"\nğŸ“¦ VÃ©rification des modÃ¨les fragmentÃ©s...")
    models_dir = Path("models")
    fragment_dirs = [d for d in models_dir.iterdir() if d.is_dir() and "fragments" in d.name]
    
    if not fragment_dirs:
        print("   âŒ Aucun modÃ¨le fragmentÃ© trouvÃ©")
        return False
    
    print(f"   ModÃ¨les fragmentÃ©s trouvÃ©s: {len(fragment_dirs)}")
    
    valid_models = []
    for model_dir in fragment_dirs:
        if verify_model_fragments(model_dir):
            valid_models.append(model_dir.name)
    
    print(f"   ModÃ¨les valides: {len(valid_models)}")
    for model in valid_models:
        print(f"     âœ… {model}")
    
    # 3. ExÃ©cuter les tests
    print(f"\nğŸ§ª ExÃ©cution des tests...")
    
    tests = [
        ([sys.executable, "tests_debug/test_serialization.py"], "Tests de sÃ©rialisation"),
        ([sys.executable, "tests_debug/test_new_manifest.py"], "Test de chargement de modÃ¨le"),
    ]
    
    test_results = []
    for cmd, description in tests:
        success = run_command(cmd, description)
        test_results.append((description, success))
    
    # 4. PrÃ©parer les fichiers de configuration
    print(f"\nğŸ“ PrÃ©paration des configurations...")
    
    # Copier les fichiers de configuration exemple
    config_files = [
        "distribution/config_example.json",
        "inference/config_example.yaml"
    ]
    
    for config_file in config_files:
        src = Path(config_file)
        if src.exists():
            dst = src.parent / f"{src.stem}.json"
            shutil.copy(src, dst)
            print(f"   âœ… Configuration copiÃ©e: {dst}")
    
    # 5. GÃ©nÃ©rer un rapport de dÃ©ploiement
    print(f"\nğŸ“Š GÃ©nÃ©ration du rapport de dÃ©ploiement...")
    
    report = {
        "status": "ready" if valid_models else "not_ready",
        "timestamp": "2024-02-17",
        "python_version": result.stdout.strip(),
        "valid_models": valid_models,
        "total_models": len(fragment_dirs),
        "test_results": {desc: "PASS" if success else "FAIL" for desc, success in test_results},
        "recommendations": []
    }
    
    if not valid_models:
        report["recommendations"].append("Fragmenter au moins un modÃ¨le avant dÃ©ploiement")
    
    if any(not success for _, success in test_results):
        report["recommendations"].append("Corriger les tests Ã©chouÃ©s avant dÃ©ploiement")
    
    # Sauvegarder le rapport
    report_path = Path("distribution/DEPLOYMENT_REPORT.json")
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"   âœ… Rapport gÃ©nÃ©rÃ©: {report_path}")
    
    # 6. Afficher le rÃ©sumÃ©
    print(f"\nğŸ“Œ RÃ©sumÃ© du dÃ©ploiement")
    print(f"   " + "=" * 56)
    print(f"   Statut: {'PRÃŠT' if valid_models and all(success for _, success in test_results) else 'NON PRÃŠT'}")
    print(f"   ModÃ¨les disponibles: {len(valid_models)}")
    print(f"   Tests passÃ©s: {sum(1 for _, success in test_results if success)}/{len(test_results)}")
    
    if valid_models:
        print(f"\n   ğŸ¯ ModÃ¨les prÃªts pour la production:")
        for model in valid_models:
            print(f"      â€¢ {model}")
    
    if report["recommendations"]:
        print(f"\n   âš ï¸  Recommandations:")
        for rec in report["recommendations"]:
            print(f"      â€¢ {rec}")
    
    # 7. Instructions de dÃ©ploiement
    print(f"\nğŸš€ Instructions de dÃ©ploiement")
    print(f"   " + "=" * 56)
    print(f"   1. VÃ©rifier que tous les tests passent")
    print(f"   2. Configurer les paramÃ¨tres rÃ©seau dans distribution/config.json")
    print(f"   3. Lancer le serveur: python distribution/server.py")
    print(f"   4. Lancer les clients: python distribution/client.py")
    print(f"   5. Monitorer avec: python tests_debug/monitor.py")
    
    return len(valid_models) > 0 and all(success for _, success in test_results)

def main():
    """Point d'entrÃ©e principal."""
    try:
        success = prepare_deployment()
        
        if success:
            print(f"\nğŸ‰ DÃ©ploiement prÃªt!")
            print(f"   Vous pouvez maintenant lancer le systÃ¨me en production.")
        else:
            print(f"\nâš ï¸  DÃ©ploiement non prÃªt.")
            print(f"   Veuillez corriger les problÃ¨mes identifiÃ©s ci-dessus.")
        
        return success
        
    except Exception as e:
        print(f"\nâŒ Erreur lors de la prÃ©paration du dÃ©ploiement: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)