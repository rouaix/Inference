# ─── Core inference ───────────────────────────────────────────────────────────
numpy>=1.24
gguf>=0.6
sentencepiece>=0.1.99
tqdm>=4.60.0          # Barres de progression

# ─── Kernels JIT (fragment_executor.py, kernels_numba.py) ────────────────────
numba>=0.59.0
psutil>=5.9

# ─── Tokenizer (extraction depuis GGUF + chargement BPE/UNIGRAM) ──────────────
tokenizers>=0.19          # HuggingFace tokenizers — BPE (Magistral, Mistral v3+)
protobuf>=4.0             # Requis par sentencepiece pour sentencepiece_model_pb2

# ─── UI ───────────────────────────────────────────────────────────────────────
gradio>=4.0

# ─── Hybrid bridge (p2p_bridge.py) ───────────────────────────────────────────
# Requires a compiled llama.cpp binary for your platform.
# Install with: pip install llama-cpp-python
llama-cpp-python>=0.3.0

# ─── distribution/reseau.py ────────────────────────────────────────────────
requests>=2.31
fastapi>=0.110
uvicorn>=0.29
aiohttp>=3.9

# ─── distribution/p2p.py (à installer quand le module sera codé) ─────────────
# py-libp2p>=0.2
# cryptography>=41.0
