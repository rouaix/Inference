# Inference-IA-P2P — Project Plan

> **Test en cours :** `Magistral-Small-2509-Q4_K_M.gguf`

## Faire tourner un gros model Mistral en P2P en réseau ou en local sans GPU.

---

## 1. Vision du projet

Permettre à n'importe qui sur PC, mobile ou tablette de contribuer à faire tourner de gros modèles de langage open-source en ne stockant que **10 Mo** de données sur son appareil pour le mode p2p.

### Principes fondateurs

- **10 Mo par utilisateur** : chaque nœud ne stocke qu'un fragment minuscule du modèle
- **Zéro GPU requis** : tout fonctionne sur CPU (ARM, x86, mobile, navigateur)
- **Multiplateforme total** : PC, Mac, Linux, Android, iOS, tablette — via une PWA ou app native légère
- **Résilient** : le réseau survit à la perte de 30 à 40% de ses nœuds
- **Installation triviale** : un clic pour rejoindre le réseau et commencer à utiliser ou contribuer
- **Open-source** : licence permissive (Apache 2.0, comme Mistral Large 3)

### Ce que ce n'est PAS

- Ce n'est pas un service temps réel : la vitesse de génération sera lente (5-60 secondes/token)
- Ce n'est pas un remplacement de ChatGPT : c'est un outil communautaire, asynchrone, décentralisé
- Ce n'est pas un projet de recherche pur : l'objectif est une app utilisable par le grand public

---

## 2. Pourquoi Mistral Large 3

### Le modèle

| Caractéristique | Valeur |
|---|---|
| Paramètres totaux | 675 milliards |
| Paramètres actifs par token | 41 milliards (6%) |
| Architecture | Mixture of Experts (MoE) granulaire |
| Experts par couche | 128 |
| Experts activés par token | 2 |
| Couches transformer | 88 |
| Dimension cachée | 6 144 |
| Fenêtre de contexte | 256 000 tokens |
| Licence | Apache 2.0 (libre, commercial autorisé) |
| Capacités | Texte, vision, multilingue, agentic, function calling |

### Pourquoi le MoE est idéal pour le P2P

L'architecture Mixture of Experts est un avantage décisif pour notre projet. Dans un modèle dense classique (comme Llama 70B), chaque token traverse **tous** les paramètres. Dans un MoE comme Mistral Large 3, chaque token n'active que **2 experts sur 128** par couche.

Conséquence directe : pour chaque requête, on ne mobilise que **~2,6% du réseau** pour les calculs d'experts. Les 97,4% restants sont en veille. Cela réduit massivement la coordination réseau et rend le système naturellement scalable.

---

## 3. Les mathématiques du découpage

### Tailles en quantification Q4_K_M (~4,5 bits/poids)

| Composant | Taille totale | Fragments (10 Mo) | Actif par token |
|---|---|---|---|
| Embedding + LM Head | ~7 Go | 700 | Toujours |
| Attention (Q/K/V/O) × 88 couches | ~5 Go | 500 | Toujours |
| Routeurs × 88 couches | ~0,3 Go | 18 | Toujours |
| Experts FFN (128 × 88 couches) | ~438 Go | 44 907 | 2/128 par couche |
| **Total** | **~450 Go** | **46 125** | **~1 218 actifs** |

### Réseau nécessaire

| Facteur de réplication | Nœuds totaux | Nœuds actifs par requête |
|---|---|---|
| ×3 (minimum) | 138 375 | ~3 654 |
| ×5 (recommandé) | 230 625 | ~6 090 |

### Impact par utilisateur

Chaque utilisateur stocke **un seul fragment de 10 Mo** sur son appareil. C'est l'équivalent de 2-3 photos. Le calcul demandé par fragment est une multiplication matricielle partielle de quelques millisecondes sur CPU.

---

## 4. Architecture technique

### 4.1. Types de fragments

Le modèle est découpé en fragments de 10 Mo classés en deux catégories :

**Fragments "toujours actifs"** (~1 218 fragments, ~12 Go) :
- Embedding et LM Head (conversion tokens ↔ vecteurs)
- Projections d'attention Q, K, V, O (cœur du raisonnement)
- Routeurs MoE (décident quels experts activer)
- Normes RMSNorm (normalisation entre couches)

Ces fragments sont sollicités pour **chaque token** généré. Ils nécessitent la réplication la plus élevée et les nœuds les plus stables.

**Fragments "experts conditionnels"** (~44 907 fragments, ~438 Go) :
- Les FFN des 128 experts × 88 couches (gate, up, down)

Pour chaque token, seuls **2 experts par couche** sont activés par le routeur. Un expert donné n'est sollicité que ~1,6% du temps. Ces fragments peuvent être sur des nœuds moins stables (mobiles, tablettes).

### 4.2. Architecture d'un nœud

```
┌─────────────────────────────────────┐
│  Application (PWA / Tauri)          │
│  ┌───────────────────────────────┐  │
│  │  Interface Chat               │  │
│  │  (envoyer prompts, voir      │  │
│  │   les réponses)               │  │
│  ├───────────────────────────────┤  │
│  │  Worker de calcul (WASM)      │  │
│  │  - Stocke 1 fragment (10 Mo) │  │
│  │  - Exécute matmul partiel    │  │
│  │  - Envoie le résultat        │  │
│  ├───────────────────────────────┤  │
│  │  Module réseau P2P            │  │
│  │  - libp2p (WebRTC)           │  │
│  │  - DHT (découverte pairs)    │  │
│  │  - Heartbeat                  │  │
│  │  - NAT traversal             │  │
│  └───────────────────────────────┘  │
│                                     │
│  Empreinte : ~15 Mo total           │
│  (10 Mo fragment + 5 Mo app)        │
└─────────────────────────────────────┘
```

### 4.3. Architecture réseau

```
                    ┌──────────────┐
                    │  Bootstrap   │
                    │  Nodes       │
                    │  (serveurs   │
                    │  d'amorçage) │
                    └──────┬───────┘
                           │
              ┌────────────┼────────────┐
              │            │            │
        ┌─────▼─────┐ ┌───▼─────┐ ┌───▼─────┐
        │  Super     │ │  Super  │ │  Super  │
        │  Node      │ │  Node   │ │  Node   │
        │  (agrège)  │ │         │ │         │
        └─────┬──────┘ └────┬────┘ └────┬────┘
          ┌───┼───┐     ┌───┼───┐   ┌───┼───┐
          │   │   │     │   │   │   │   │   │
          n1  n2  n3    n4  n5  n6  n7  n8  n9
          │   │   │     │   │   │   │   │   │
         10Mo chacun   10Mo chacun  10Mo chacun
```

**Bootstrap Nodes** : quelques serveurs stables qui aident les nouveaux nœuds à rejoindre le réseau. Ne participent pas au calcul, juste à la découverte.

**Super Nodes** : nœuds élus dynamiquement (bon uptime, bonne bande passante) qui agrègent les résultats partiels d'un groupe de nœuds. Réduisent le nombre de connexions nécessaires.

**Nœuds réguliers** : les utilisateurs ordinaires. Stockent un fragment, font leur calcul, envoient le résultat.

### 4.4. Flux d'inférence détaillé

```
Utilisateur tape : "Explique la photosynthèse"
        │
        ▼
   1. TOKENISATION (locale)
      "Explique la photosynthèse" → [15234, 432, 98712]
        │
        ▼
   2. EMBEDDING (fragments toujours actifs)
      tokens → vecteurs de dimension 6144
      Nœuds contactés : ~70 (700 fragments / 10 par fragment, répliques)
        │
        ▼
   3. COUCHE 1/88 — ATTENTION
      │
      ├─ Q_proj : 5 nœuds calculent en parallèle
      ├─ K_proj : 2 nœuds
      ├─ V_proj : 2 nœuds
      ├─ Attention computation (agrégateur)
      └─ O_proj : 5 nœuds
      │
      ├─ ROUTEUR : 1 nœud évalue les scores des 128 experts
      │            → sélectionne Expert 42 et Expert 97
      │
      ├─ EXPERT 42 : 5 nœuds (gate + up + down)
      ├─ EXPERT 97 : 5 nœuds (gate + up + down)
      │
      └─ Agrégation pondérée des résultats experts
        │
        ▼
   4. COUCHES 2 à 88 — même processus
        │
        ▼
   5. LM HEAD (fragments toujours actifs)
      vecteur 6144 → logits sur 131 072 tokens du vocabulaire
      → token le plus probable sélectionné
        │
        ▼
   6. DÉCODAGE
      token_id → "La"
        │
        ▼
   7. BOUCLE → retour à l'étape 2 pour le token suivant
      jusqu'à complétion de la réponse
```

### 4.5. Gestion du contexte (fenêtre d'entrée)

Mistral Large 3 supporte une fenêtre de **256K tokens** (~200 pages de texte). Le KV cache associé est volumineux :

| Longueur contexte | Taille KV cache (Q4) | Stratégie |
|---|---|---|
| 2K tokens | ~8 Mo | Distribué par couche, ~90 Ko/couche |
| 8K tokens | ~32 Mo | Distribué par couche, ~360 Ko/couche |
| 32K tokens | ~128 Mo | Distribué par couche, ~1,5 Mo/couche |
| 256K tokens | ~1 Go | Nécessite des nœuds dédiés au cache |

**Stratégie retenue** : chaque groupe de nœuds responsable d'une couche conserve le KV cache de cette couche. La réplication assure que si un nœud tombe, une réplique reprend avec son propre cache.

Pour le MVP, on limite le contexte à **8K tokens** (~6 pages), ce qui est suffisant pour la plupart des conversations.

---

## 5. Résilience et tolérance aux pannes

### 5.1. Stratégie de réplication

Chaque fragment est hébergé par N nœuds simultanément :

| Réplication | Survit sans dégradation | Survit en mode dégradé |
|---|---|---|
| ×3 | Jusqu'à ~10% de pannes | Jusqu'à ~30% de pannes |
| ×5 | Jusqu'à ~40% de pannes | Jusqu'à ~50% de pannes |

**Mode dégradé** : certains experts sont indisponibles. Le routeur redirige vers d'autres experts disponibles. La qualité du texte baisse légèrement, mais l'inférence continue.

### 5.2. Mécanismes de résilience

**Heartbeat** : chaque nœud envoie un signal "je suis vivant" toutes les 30 secondes. Si 3 heartbeats manqués → le nœud est déclaré mort.

**Failover automatique** : quand un nœud tombe en plein calcul, la requête est re-routée vers une réplique. Latence ajoutée : ~200-500ms.

**Redistribution dynamique** : quand un fragment n'a plus assez de répliques (sous le seuil de sécurité), le réseau demande à d'autres nœuds de le télécharger et de le servir.

**Remplacement d'experts** : spécifique au MoE. Si un expert est totalement indisponible, le routeur peut augmenter le poids des experts restants. La qualité baisse marginalement.

### 5.3. Priorité de stabilité des nœuds

Les fragments "toujours actifs" sont critiques. Ils sont assignés en priorité aux nœuds les plus stables (PC fixes, serveurs, bonne connexion). Les fragments experts (conditionnels) peuvent aller sur des nœuds plus volatils (mobiles, tablettes).

Le réseau mesure la stabilité de chaque nœud (uptime historique, latence moyenne) et ajuste les assignations en conséquence.

---

## 6. Stack technique

### 6.1. Moteur de calcul

| Composant | Technologie | Justification |
|---|---|---|
| Multiplication matricielle | llama.cpp compilé en WASM | Tourne partout (navigateur, mobile, desktop), optimisé CPU |
| Format des poids | GGUF quantifié Q4_K_M | Standard de facto, bon ratio qualité/taille |
| Runtime WASM | wasmtime (natif) ou navigateur | Performance proche du natif |

### 6.2. Réseau P2P

| Composant | Technologie | Justification |
|---|---|---|
| Transport P2P | libp2p | Mature, supporte WebRTC (navigateur-à-navigateur) |
| NAT traversal | WebRTC ICE + TURN relays | Fonctionne derrière les box internet |
| Découverte | DHT Kademlia | Prouvé, utilisé par BitTorrent/IPFS |
| Sérialisation | Protocol Buffers / MessagePack | Compact, rapide |

### 6.3. Interface utilisateur

| Plateforme | Solution | Taille estimée |
|---|---|---|
| Desktop (Windows/Mac/Linux) | Tauri (Rust + WebView) | ~5 Mo installeur |
| Mobile / Tablette | PWA installable | 0 Mo installeur (navigateur) |
| Web pur | Page web + Service Worker | 0 Mo |

### 6.4. Langages

| Couche | Langage | Raison |
|---|---|---|
| Moteur d'inférence | C++ (llama.cpp) → compilé WASM | Performance maximale |
| Réseau P2P | Rust (libp2p) ou TypeScript (js-libp2p) | Fiabilité / portabilité |
| App desktop | Rust (Tauri) | Léger, cross-platform |
| Interface | TypeScript + Svelte ou React | Réactivité, écosystème |
| Orchestrateur | Rust ou Go | Performance réseau, concurrence |

---

## 7. Feuille de route

### Phase 1 — Proof of Concept (FAIT ✅)

**Objectif** : valider que le découpage et la simulation fonctionnent.

- [x] Modéliser l'architecture MoE de Mistral Large 3
- [x] Fragmenteur : découpe un modèle simulé en chunks de 10 Mo
- [x] Simulateur de réseau P2P avec nœuds, réplication, DHT
- [x] Distinction fragments "toujours actifs" vs "experts conditionnels"
- [x] Tests de tolérance aux pannes (0% à 70% de nœuds perdus)
- [x] Calcul des stats réelles pour Mistral Large 3 (46 125 fragments)

**Résultat** : le concept est validé. Avec réplication ×5, le réseau survit à 40% de pannes sans dégradation.

### Phase 2 — Fragmenteur GGUF réel

**Objectif** : découper un vrai modèle Mistral au format GGUF.

- [ ] Parser le format GGUF (header, métadonnées, tenseurs)
- [ ] Identifier les couches attention, experts, routeurs dans le fichier
- [ ] Découper les tenseurs en fragments de 10 Mo avec métadonnées correctes
- [ ] Reconstruire un modèle à partir des fragments et vérifier l'intégrité (bit-perfect)
- [ ] Tester sur Mistral 7B v0.3 GGUF (4 Go, ~400 fragments)
- [ ] Tester sur Mistral Large 3 GGUF (450 Go, ~46 000 fragments)
- [ ] Générer le manifeste complet (mapping fragment ↔ couche ↔ expert)

**Livrable** : un outil CLI qui prend un fichier GGUF en entrée et produit N fragments de 10 Mo + un manifeste JSON.

### Phase 3 — Calcul distribué réel

**Objectif** : faire une vraie inférence en coordonnant plusieurs processus.

- [ ] Implémenter la multiplication matricielle partielle sur un fragment
- [ ] Tester : N processus locaux, chacun avec un fragment, qui collaborent via IPC
- [ ] Vérifier que le résultat agrégé est identique à llama.cpp standard
- [ ] Implémenter le pipeline couche par couche avec passage d'activations
- [ ] Implémenter la sélection d'experts par le routeur
- [ ] Benchmarker : temps par token en fonction du nombre de fragments
- [ ] Gérer le KV cache distribué (chaque groupe de nœuds garde son cache)

**Livrable** : un système multi-processus local qui fait de l'inférence correcte sur Mistral 7B découpé.

### Phase 4 — Réseau P2P réel

**Objectif** : passer du multi-processus local au vrai P2P sur internet.

- [ ] Intégrer libp2p (transport WebRTC + TCP)
- [ ] Implémenter la DHT Kademlia pour la découverte de nœuds
- [ ] Bootstrap nodes : serveurs d'amorçage pour les nouveaux arrivants
- [ ] NAT traversal : hole punching, TURN relay pour les cas difficiles
- [ ] Protocole de communication : envoi d'activations, réception de résultats partiels
- [ ] Heartbeat et détection de pannes
- [ ] Failover automatique vers les répliques
- [ ] Groupement géographique pour minimiser la latence
- [ ] Tester avec 10-50 machines réelles sur internet

**Livrable** : un réseau P2P fonctionnel capable de faire de l'inférence distribuée entre machines distantes.

### Phase 5 — Application utilisateur

**Objectif** : une app que n'importe qui peut installer et utiliser.

- [ ] App desktop Tauri (Windows/Mac/Linux)
  - Installeur one-click (<10 Mo)
  - Interface chat simple
  - Indicateur de statut réseau
  - Choix : "je veux utiliser" / "je veux contribuer" / "les deux"
- [ ] PWA pour mobile/tablette
  - Fonctionne dans le navigateur
  - Installable sur l'écran d'accueil
  - Service Worker pour le fonctionnement en arrière-plan
- [ ] Onboarding
  - Premier lancement : téléchargement automatique d'un fragment (10 Mo)
  - Assignation intelligente du fragment (en fonction des besoins du réseau)
  - Aucune configuration technique requise
- [ ] Dashboard réseau
  - Nombre de nœuds en ligne
  - Santé du réseau (% de couverture)
  - Stats personnelles (contributions, uptime)

**Livrable** : application publique téléchargeable/utilisable, avec onboarding guidé.

### Phase 6 — Scaling et optimisation

**Objectif** : passer de quelques centaines à des dizaines de milliers de nœuds.

- [ ] Optimisation de la bande passante (compression des activations)
- [ ] Système d'incitation (crédits de calcul : tu contribues → tu utilises)
- [ ] Architecture en arbre pour l'agrégation (réduire les allers-retours)
- [ ] Cache intelligent (les requêtes fréquentes sont pré-calculées)
- [ ] Support de plusieurs modèles (Mistral 7B, Small 3.2, Large 3)
- [ ] API pour les développeurs (comme Petals actuel mais en plus simple)
- [ ] Monitoring et alertes (santé du réseau, fragments sous-répliqués)
- [ ] Mode asynchrone avancé (file d'attente de requêtes, notification quand c'est prêt)

---

## 8. Estimations de performance

### Temps par token (estimations conservatrices)

| Scénario | Latence réseau | Calcul CPU | Total/token |
|---|---|---|---|
| Réseau local (LAN) | ~1 ms/hop | ~5 ms | ~5-10 secondes |
| Internet fibre | ~20 ms/hop | ~5 ms | ~15-30 secondes |
| Internet mixte (mobile) | ~50-100 ms/hop | ~10 ms | ~30-60 secondes |

### Pourquoi c'est lent (et pourquoi c'est acceptable)

L'inférence traverse 88 couches séquentiellement. Chaque couche nécessite des dizaines d'allers-retours réseau (envoi activations → calcul distribué → agrégation → couche suivante). Avec 88 couches et ~20ms par aller-retour, on arrive à ~2 secondes juste pour le réseau, sans compter le calcul.

C'est viable pour un usage **asynchrone** : l'utilisateur pose sa question, fait autre chose, revient lire la réponse quelques minutes plus tard. Comparable à envoyer un email plutôt qu'un message instantané.

### Temps de génération pour une réponse complète

| Longueur réponse | Tokens | Temps estimé (fibre) | Temps estimé (mixte) |
|---|---|---|---|
| Phrase courte | ~30 tokens | ~8 minutes | ~15 minutes |
| Paragraphe | ~100 tokens | ~25 minutes | ~50 minutes |
| Réponse détaillée | ~500 tokens | ~2 heures | ~4 heures |

---

## 9. Système d'incitation

### Problème

Pourquoi les gens garderaient-ils l'app ouverte et contribueraient-ils leur CPU et bande passante ?

### Solution : crédits de calcul

**Principe simple** : tu contribues du calcul → tu gagnes des crédits → tu utilises tes crédits pour poser des questions.

| Action | Crédits |
|---|---|
| Héberger un fragment (par heure d'uptime) | +1 crédit |
| Répondre à une requête de calcul | +0.1 crédit |
| Poser une question au modèle | -10 à -50 crédits (selon longueur) |
| Nouveau utilisateur (bonus bienvenue) | +100 crédits |

Les utilisateurs qui contribuent beaucoup (nœuds stables, bon uptime) accumulent des crédits passivement. Ceux qui veulent juste utiliser le modèle sans contribuer peuvent le faire en consommant leurs crédits de bienvenue, puis doivent laisser l'app tourner.

### Alternative : mode altruiste

Certains utilisateurs voudront simplement contribuer sans contrepartie (comme les seeders BitTorrent). L'app affiche leur contribution et un "merci" public (pseudo + stats sur le dashboard réseau).

---

## 10. Sécurité et confidentialité

### Menaces identifiées

| Menace | Impact | Mitigation |
|---|---|---|
| Nœud malveillant qui renvoie de faux résultats | Réponse corrompue | Calcul redondant : 2+ nœuds font le même calcul, comparaison des résultats |
| Interception des activations (prompt sniffing) | Fuite de données | Chiffrement des activations en transit (TLS/DTLS) |
| Nœud qui espionne les prompts | Vie privée | Aucun nœud ne voit le prompt complet, seulement des vecteurs numériques intermédiaires |
| Attaque Sybil (faux nœuds) | Prise de contrôle | Système de réputation basé sur l'historique d'uptime et la cohérence des calculs |
| Déni de service | Réseau inutilisable | Rate limiting, priorité aux nœuds avec bonne réputation |

### Confidentialité inhérente au design

Un point fort du design : **aucun nœud individuel ne voit le texte en clair**. Chaque nœud ne reçoit que des vecteurs d'activation (des tableaux de nombres flottants). Seul le nœud de l'utilisateur qui a posé la question fait la tokenisation et le décodage.

Pour les cas sensibles, il sera possible de lancer un **swarm privé** entre des nœuds de confiance.

---

## 11. Comparaison avec l'existant

| | Ollama / llama.cpp | API cloud (OpenAI, etc.) |
|---|---|---|---|---|
| GPU requis | Oui (serveurs) | Non | Recommandé | Non (cloud) |
| Fragment par nœud | Couches entières (~1-5 Go) | 10 Mo | Modèle entier | N/A |
| Installation | pip + Python + CLI | Un clic / PWA | CLI + téléchargement | Clé API |
| Mobile | Non | Oui | Non | Via app |
| Nœuds nécessaires | ~10-50 | ~50 000-200 000 | 1 | 0 (centralisé) |
| Vitesse | ~4-6 tokens/s | ~0.02-0.1 token/s | ~10-30 tokens/s | ~50-100 tokens/s |
| Coût | Gratuit | Gratuit | Gratuit | Payant |
| Vie privée | Moyenne (nœuds voient les couches) | Bonne (fragments opaques) | Excellente (local) | Faible (cloud) |
| Censure | Résistant | Très résistant | Local | Censuré |
| Dernier modèle supporté | Llama 3.1 405B | Mistral Large 3 675B | Variable | Derniers modèles |

---

## 12. Risques et mitigations

| Risque | Probabilité | Impact | Mitigation |
|---|---|---|---|
| Pas assez de nœuds pour couvrir le modèle | Haute (au début) | Bloquant | Commencer par Mistral 7B (400 fragments), monter progressivement |
| Latence trop élevée → inutilisable | Moyenne | Majeur | Mode asynchrone, file d'attente, notifications |
| Calcul distribué donne des résultats incorrects | Moyenne | Majeur | Tests exhaustifs Phase 3, vérification bit-perfect vs llama.cpp |
| WebRTC/NAT traversal ne fonctionne pas partout | Moyenne | Modéré | Fallback TURN relay, nœuds relais |
| Utilisateurs ne restent pas connectés | Haute | Modéré | Incitations, gamification, app légère en arrière-plan |
| Modèle trop gros même découpé | Faible | Modéré | Support multi-modèle, commencer petit |

---

## 13. MVP minimal viable

Le plus petit déploiement fonctionnel qui prouve le concept en conditions réelles :

**Modèle** : Mistral 7B v0.3 quantifié Q4_K_M (~4 Go, ~400 fragments de 10 Mo)

**Nœuds** : 400 × 3 répliques = 1 200 nœuds minimum

**Fonctionnalités** :
- Découper le vrai GGUF en fragments
- Réseau P2P fonctionnel (libp2p, WebRTC)
- Inférence distribuée correcte (vérifié vs llama.cpp)
- Interface chat web basique
- Dashboard réseau (nombre de nœuds, santé)

**Critère de succès** : générer une réponse cohérente de 50 tokens à partir d'un prompt, avec au moins 10% des nœuds sur des machines différentes, et survivre à la perte de 2 nœuds pendant la génération.

---

## 14. Structure du code (prévue)

```
inferencep2pia/
├── README.md
├── project.md                    ← ce document
│
├── core/                         # Cœur du système
│   ├── fragmenter/               # Découpage GGUF → fragments
│   │   ├── gguf_parser.py        # Parser de fichiers GGUF
│   │   ├── tensor_splitter.py    # Découpe des tenseurs
│   │   └── manifest.py           # Génération du manifeste
│   │
│   ├── inference/                # Moteur d'inférence distribué
│   │   ├── engine.py             # Orchestrateur d'inférence
│   │   ├── matmul_partial.py     # Multiplication matricielle partielle
│   │   ├── moe_router.py         # Routage MoE distribué
│   │   ├── kv_cache.py           # Gestion du KV cache distribué
│   │   └── aggregator.py         # Agrégation des résultats partiels
│   │
│   └── network/                  # Couche réseau P2P
│       ├── node.py               # Logique d'un nœud
│       ├── dht.py                # Table de hachage distribuée
│       ├── transport.py          # WebRTC / TCP transport
│       ├── heartbeat.py          # Détection de pannes
│       └── replication.py        # Gestion de la réplication
│
├── app/                          # Application utilisateur
│   ├── desktop/                  # App Tauri (Windows/Mac/Linux)
│   ├── pwa/                      # Progressive Web App (mobile/web)
│   └── shared/                   # Composants UI partagés
│
├── infra/                        # Infrastructure
│   ├── bootstrap/                # Serveurs d'amorçage
│   ├── relay/                    # Nœuds relais TURN
│   └── monitor/                  # Dashboard de monitoring
│
├── tests/                        # Tests
│   ├── unit/                     # Tests unitaires
│   ├── integration/              # Tests d'intégration
│   └── simulation/               # Tests de simulation réseau
│
├── poc/                          # Proof of Concept (Phase 1)
│   ├── fragmenter.py             # Fragmenteur v1
│   ├── fragmenter_v2.py          # Fragmenteur v2 (MoE-aware)
│   ├── simulator.py              # Simulateur v1
│   └── simulator_v2.py           # Simulateur v2 (MoE-aware)
│
└── docs/                         # Documentation
    ├── architecture.md           # Architecture technique détaillée
    ├── protocol.md               # Protocole réseau
    ├── contributing.md           # Guide de contribution
    └── faq.md                    # Questions fréquentes
```

---

## 15. Contacts et ressources

- **Mistral Large 3** : [HuggingFace](https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512) — Apache 2.0
- **Petals (inspiration)** : [GitHub](https://github.com/bigscience-workshop/petals) — MIT
- **llama.cpp** : [GitHub](https://github.com/ggerganov/llama.cpp) — MIT
- **libp2p** : [Site](https://libp2p.io/) — MIT/Apache 2.0
- **Tauri** : [Site](https://tauri.app/) — MIT/Apache 2.0
- **Paper Petals** : [arXiv](https://arxiv.org/abs/2209.01188)
- **Format GGUF** : [Spec](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
