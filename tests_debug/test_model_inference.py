#!/usr/bin/env python3
"""
Test d'inf√©rence simple pour v√©rifier que les mod√®les fragment√©s fonctionnent.
"""

import sys
import os
from pathlib import Path

# Ajouter le chemin du projet (remonter de tests_debug/ vers la racine)
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

def test_model_inference(model_dir_name):
    """Test l'inf√©rence avec un mod√®le fragment√©."""
    print(f"\n{'='*60}")
    print(f"Test d'inf√©rence: {model_dir_name}")
    print(f"{'='*60}")
    
    try:
        from inference.p2p_inference import P2PInferenceEngine
        
        # Charger le mod√®le
        model_dir = Path(f"models/{model_dir_name}")
        if not model_dir.exists():
            print(f"[ERREUR] Dossier non trouve: {model_dir}")
            return False
        
        # V√©rifier que le manifest existe
        manifest_path = model_dir / "manifest.json"
        if not manifest_path.exists():
            print(f"[ERREUR] Manifest manquant: {manifest_path}")
            print(f"[INFO] Ce modele n'est pas completement fragmente ou le manifest n'a pas ete genere")
            return False
        
        print(f"Chargement du mod√®le depuis {model_dir}...")
        engine = P2PInferenceEngine(str(model_dir), verbose=True)
        
        # Afficher la configuration
        cfg = engine.config
        print(f"\nConfiguration du mod√®le:")
        print(f"  Couches: {cfg.n_layers}")
        print(f"  Dimensions: {cfg.dim}")
        print(f"  T√™tes: {cfg.n_heads} (KV: {cfg.n_kv_heads})")
        print(f"  Vocabulaire: {cfg.vocab_size}")
        print(f"  FFN: {cfg.hidden_dim}")
        
        # Tester le tokenizer
        print(f"\nTest du tokenizer:")
        test_prompt = "Bonjour, comment √ßa va?"
        tokens = engine.tokenizer.encode(test_prompt)
        print(f"  Prompt: '{test_prompt}'")
        print(f"  Tokens: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        print(f"  Nombre de tokens: {len(tokens)}")
        
        # D√©coder les tokens
        decoded = engine.tokenizer.decode(tokens)
        print(f"  D√©codage: '{decoded}'")
        
        # Tester le chargement de quelques tenseurs cl√©s
        print(f"\nTest de chargement des tenseurs:")
        key_tensors = [
            "token_embd.weight",
            "output.weight",
            "blk.0.attn_q.weight",
            "blk.0.attn_k.weight",
            "blk.0.attn_v.weight",
            "blk.0.attn_output.weight",
            "blk.0.ffn_gate.weight",
            "blk.0.ffn_up.weight",
            "blk.0.ffn_down.weight",
        ]
        
        loaded_successfully = 0
        for tensor_name in key_tensors:
            try:
                tensor = engine.load_tensor(tensor_name)
                print(f"  [OK] {tensor_name:<30} shape={tensor.shape}")
                loaded_successfully += 1
            except Exception as e:
                print(f"  [ERREUR] {tensor_name:<30} ERREUR: {e}")
        
        # Tester un forward pass simple (un seul token)
        print(f"\nTest de forward pass (token unique):")
        try:
            # Utiliser le token BOS (d√©but de s√©quence)
            bos_token = 1  # Typiquement 1 pour les mod√®les Llama/Mistral
            
            # Charger l'embedding
            embeddings = engine.load_tensor("token_embd.weight")
            if embeddings.ndim == 2 and embeddings.shape[1] == cfg.vocab_size:
                # Transposer si n√©cessaire
                embeddings = embeddings.T
            
            # Obtenir l'embedding pour le token BOS
            x = embeddings[bos_token].reshape(1, -1)
            print(f"  Embedding BOS shape: {x.shape}")
            print(f"  Embedding stats: mean={x.mean():.4f}, std={x.std():.4f}")
            
            # Passer √† travers la premi√®re couche
            from inference.p2p_inference import LlamaLayer
            layer0 = LlamaLayer(engine, 0)
            
            # Forward pass (sans cache pour simplifier)
            output, _, _ = layer0.forward(x, engine.freqs_cis, None, None, 0)
            print(f"  Sortie couche 0 shape: {output.shape}")
            print(f"  Sortie stats: mean={output.mean():.4f}, std={output.std():.4f}")
            
            # V√©rifier les NaN/Inf
            has_nan = bool(np.any(np.isnan(output)))
            has_inf = bool(np.any(np.isinf(output)))
            
            if has_nan:
                print(f"  ‚ö†Ô∏è  Avertissement: NaN d√©tect√©s dans la sortie")
            if has_inf:
                print(f"  ‚ö†Ô∏è  Avertissement: Inf d√©tect√©s dans la sortie")
            
            if not has_nan and not has_inf:
                print(f"  [OK] Forward pass reussi sans NaN/Inf")
                forward_success = True
            else:
                print(f"  [ERREUR] Forward pass a produit des valeurs numeriques invalides")
                forward_success = False
                
        except Exception as e:
            print(f"  [ERREUR] Echec du forward pass: {e}")
            import traceback
            traceback.print_exc()
            forward_success = False
        
        # R√©sum√©
        print(f"\n{'='*60}")
        print(f"R√©sum√© pour {model_dir_name}:")
        print(f"  Tenseurs charg√©s: {loaded_successfully}/{len(key_tensors)}")
        print(f"  Forward pass: {'PASS' if forward_success else 'FAIL'}")
        print(f"  Statut global: {'‚úÖ PR√äT' if loaded_successfully == len(key_tensors) and forward_success else '‚ùå PROBL√àMES'}")
        
        return loaded_successfully == len(key_tensors) and forward_success
        
    except Exception as e:
        print(f"Erreur lors du test: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Teste tous les mod√®les fragment√©s disponibles."""
    print("Test d'inf√©rence pour les mod√®les fragment√©s")
    print("=" * 60)
    
    # Trouver tous les dossiers de fragments
    models_dir = Path("models")
    fragment_dirs = [d for d in models_dir.iterdir() if d.is_dir() and "fragments" in d.name]
    
    print(f"Mod√®les fragment√©s trouv√©s: {len(fragment_dirs)}")
    for d in fragment_dirs:
        print(f"  - {d.name}")
    
    # Tester chaque mod√®le
    results = {}
    for model_dir in fragment_dirs:
        success = test_model_inference(model_dir.name)
        results[model_dir.name] = success
    
    # R√©sum√© final
    print(f"\n{'='*60}")
    print("R√âSUM√â DES TESTS D'INF√âRENCE")
    print(f"{'='*60}")
    
    total_models = len(results)
    successful = sum(1 for success in results.values() if success)
    
    print(f"Mod√®les test√©s: {total_models}")
    print(f"Succ√®s: {successful}")
    print(f"√âchecs: {total_models - successful}")
    
    for model_name, success in results.items():
        status = "‚úÖ" if success else "‚ùå"
        print(f"{status} {model_name}")
    
    if successful == total_models:
        print(f"\nüéâ Tous les mod√®les passent les tests d'inf√©rence!")
        print(f"   Les mod√®les sont pr√™ts pour l'inf√©rence en production.")
    else:
        print(f"\n‚ö†Ô∏è  Certains mod√®les ont √©chou√© les tests d'inf√©rence.")
        print(f"   Veuillez v√©rifier les erreurs ci-dessus.")
    
    return successful == total_models

if __name__ == "__main__":
    # Importer numpy pour le test
    import numpy as np
    success = main()
    sys.exit(0 if success else 1)