# INFOS.md

Documentation du projet **rouaix.com/inference** ‚Äî syst√®me d'inf√©rence P2P distribu√©.

---

## Objectif du projet

Syst√®me d'inf√©rence distribu√© pair-√†-pair permettant de faire tourner de grands mod√®les de langage (LLM) sans serveur centralis√©. Chaque n≈ìud du r√©seau stocke seulement **10 Mo** du mod√®le. Actuellement test√© avec **TinyLlama-1.1B-Chat Q8_0**, con√ßu pour supporter **Mistral Large 3 (675B)**.

---

## Environnement de d√©veloppement

Toutes les commandes doivent utiliser le virtualenv Python du projet :

```bash
# Installation initiale
.venv\Scripts\python.exe -m pip install -r requirements.txt

# Ex√©cuter un script
.venv\Scripts\python.exe <script.py>

# Ou activer le virtualenv une fois pour toute la session
.venv\Scripts\activate.bat
```

Les mod√®les et fragments sont dans `models/`. Jeu de fragments actif : `models/tinyllama_q8_fragments_v2/`.

---

## Commandes courantes

```bash
# Inf√©rence ‚Äî moteur Python pur (lent, pour d√©bogage)
.venv\Scripts\python.exe p2p_inference.py models/tinyllama_q8_fragments_v2 --prompt "Hello" --max-tokens 20 --temperature 0.7

# Inf√©rence ‚Äî bridge llama.cpp (pr√©f√©r√© en production)
.venv\Scripts\python.exe p2p_bridge.py models/tinyllama_q8_fragments_v2 --prompt "Hello"

# Fragmenter un nouveau mod√®le GGUF
.venv\Scripts\python.exe fragmenter.py models/model.gguf --output models/model_fragments

# Lancer l'interface Gradio
launch_ui.bat   # ou : .venv\Scripts\python.exe app.py

# Tester le chargeur de fragments local
.venv\Scripts\python.exe distribution\local.py models/tinyllama_q8_fragments_v2 --tensor blk.0.attn_q.weight

# Tests unitaires rapides
.venv\Scripts\python.exe tests_debug/validate_inference.py models/tinyllama_q8_fragments_v2 --units-only

# Validation compl√®te contre la r√©f√©rence llama.cpp
.venv\Scripts\python.exe tests_debug/validate_inference.py models/tinyllama_q8_fragments_v2 --gguf models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf
```

---

## Architecture

### Flux de donn√©es

```
Fichier GGUF ‚Üí fragmenter.py ‚Üí fragments *.dat + manifest.json
                                          ‚Üì
                             distribution/ (couche de chargement)
                             ‚îú‚îÄ‚îÄ local.py   ‚úÖ impl√©ment√©
                             ‚îú‚îÄ‚îÄ reseau.py  üöß stub document√©
                             ‚îî‚îÄ‚îÄ p2p.py     üöß stub document√©
                                          ‚Üì
                              P2PInferenceEngine (p2p_inference.py)
                              OU P2PBridge (p2p_bridge.py)
                                          ‚Üì
                                    app.py (Interface Gradio)
```

### Composants principaux

**`distribution/`**
Couche d'abstraction pour le chargement des fragments. `BaseFragmentLoader` d√©finit l'interface commune :
- `load_raw(fragment_id) ‚Üí bytes` ‚Äî lit les octets bruts d'un fragment
- `load_tensor(tensor_name) ‚Üí np.ndarray` ‚Äî reconstitue et dequantise un tenseur

`LocalFragmentLoader` est le seul backend fonctionnel. `ReseauFragmentLoader` et `P2PFragmentLoader` sont des stubs document√©s qui l√®vent `NotImplementedError`. La logique de dequantisation Q8_0 (avec la correction du layout transpos√©) est dans `LocalFragmentLoader._dequantize_q8_0`.

**`p2p_inference.py`**
Impl√©mentation NumPy pure du transformer LLaMA. Utile pour l'apprentissage et le d√©bogage. Lent (~14s/token, sans cache KV, sans batching). Contient : `P2PInferenceEngine`, `LlamaLayer`, `ModelConfig`, utilitaires de sampling. Poss√®de encore son propre `load_tensor` (logique identique √† `LocalFragmentLoader`, pas encore fusionn√©e).

**`fragmenter.py`**
D√©coupe n'importe quel fichier GGUF en morceaux de 10 Mo (`.dat`). G√©n√®re un `manifest.json` indexant chaque tenseur vers ses fragments. G√®re les types Q8_0 et F32.

**`p2p_bridge.py`**
Chemin de production : reconstruit le GGUF en m√©moire depuis les fragments, puis lance l'inf√©rence via `llama-cpp-python`. R√©sultats num√©riquement identiques √† llama.cpp.

**`app.py`**
Interface Gradio 6.x (6 onglets). Importe `rms_norm`, `LlamaLayer`, `_sample_logits` directement depuis `p2p_inference.py`. L'onglet **Mod√®le** inclut un s√©lecteur de mode de distribution (Local / R√©seau / P2P) ‚Äî les modes non-local affichent un message d'attente. `find_default_fragments_dir()` d√©tecte automatiquement un dossier de fragments dans `models/`, `.` ou `..` pour pr√©-remplir le chemin au d√©marrage.

**`recombiner.py`**
Inverse de `fragmenter.py` : reconstruit un fichier GGUF complet √† partir des fragments et du manifest. Utilise la biblioth√®que `gguf` pour r√©√©crire les tenseurs. Sert √† la v√©rification d'int√©grit√©.

**`tests_debug/validate_inference.py`**
Suite de validation. Tests unitaires pour RMSNorm, softmax, SwiGLU, RoPE. Compare les logits contre la r√©f√©rence llama.cpp.

**`simulation/`**
Scripts de preuve de concept Phase 1 (`fragmenter_v2.py`, `simulator_v2.py`). Simulent la fragmentation MoE et le comportement d'un r√©seau P2P sans vrai mod√®le. Hors du chemin de production.

---

## Format du manifest de fragments

Chaque tenseur dans `manifest.json` est d√©crit ainsi :

```json
{
  "tensor_name": "blk.0.attn_q.weight",
  "tensor_type": "Q8_0",
  "shape": [2048, 2048],
  "dtype": "uint8",
  "fragment_id": "tinyllama_L0_attn_q_S0_abc123",
  "shard_index": 0,
  "total_shards": 1
}
```

---

## Points d'impl√©mentation critiques

### Dequantisation Q8_0 ‚Äî Layout physique transpos√©

**C'est le d√©tail le plus important et le moins √©vident du projet.**

GGUF stocke les tenseurs Q8_0 avec un layout physique transpos√© :
- Shape logique dans les m√©tadonn√©es : `[in_dim, out_dim]`
- Layout physique des donn√©es : `[out_dim, in_dim]` (une ligne par unit√© de sortie)

Correction appliqu√©e dans `load_tensor()` :

```python
if len(shape) == 2:
    out_dim = shape[-1]  # 2e dim logique = nb de lignes physiques
    in_dim  = shape[0]   # 1re dim logique = √©l√©ments par ligne physique
    res = decoded.reshape([out_dim, in_dim]).T.astype(np.float32)  # ‚Üí [in_dim, out_dim]
```

Apr√®s cette correction, toutes les matrices de poids sont en format `[in, out]` et utilis√©es directement via `x @ w`. **Ne pas retransposer** dans `proj()` ou ailleurs.

Ce bug √©tait la cause du boucle de token ">>" (pr√©diction syst√©matique du token 5099 quel que soit le contexte). Avant la correction, la corr√©lation avec llama.cpp √©tait de ~0.009.

### R√©solution du tokenizer

`P2PInferenceEngine.__init__()` cherche `tokenizer.model` dans l'ordre suivant :
1. `fragments_dir/tokenizer.model`
2. `fragments_dir.parent/tokenizer.model` ‚Üê n√©cessaire car le tokenizer est dans `models/tokenizer.model`
3. `./tokenizer.model` (dossier courant)

### Template de chat TinyLlama

Pour des sorties coh√©rentes avec TinyLlama-1.1B-Chat :

```
<|system|>
You are a helpful assistant.</s>
<|user|>
{prompt}</s>
<|assistant|>
```

Sans ce template, le mod√®le produit des tokens arbitraires.

### Prefill ‚Äî traitement de la s√©quence compl√®te

Dans `generate()`, chaque √©tape autoregressive retraite **toute la s√©quence** (`prompt + tokens g√©n√©r√©s`) depuis la position 0. Il n'y a pas de cache KV ‚Äî c'est O(n¬≤) mais math√©matiquement correct. Ne pas optimiser en ne passant que le dernier token.

### Projection des poids dans LlamaLayer

```python
def proj(inp, w, out_dim):
    if w.ndim == 2 and w.shape[0] == out_dim and w.shape[1] != out_dim:
        return inp @ w.T   # fallback : poids en [out, in]
    return inp @ w         # standard : poids en [in, out]
```

Apr√®s la correction Q8_0, tous les poids sont en `[in, out]`. Le premier branchement ne devrait jamais s'activer pour les tenseurs Q8_0.

### GQA (Grouped Query Attention)

TinyLlama utilise `n_heads=32`, `n_kv_heads=4`. Les t√™tes KV sont r√©p√©t√©es 8√ó avant l'attention :

```python
n_rep = cfg.n_heads // cfg.n_kv_heads  # = 8
keys   = np.repeat(xk, n_rep, axis=1)
values = np.repeat(xv, n_rep, axis=1)
```

---

## Configuration du mod√®le TinyLlama Q8_0

| Param√®tre | Valeur |
|-----------|--------|
| dim | 2048 |
| hidden_dim | 5632 |
| n_layers | 22 |
| n_heads | 32 |
| n_kv_heads | 4 |
| vocab_size | 32000 |
| norm_eps | 1e-5 |
| rope_freq_base | 10000.0 |

---

## Feuille de route

| Phase | Statut | Description |
|-------|--------|-------------|
| Phase 1 | ‚úÖ Termin√© | Simulation PoC (mod√®le MoE, r√©seau P2P, tol√©rance aux pannes) |
| Phase 2 | üöß En cours | Fragmenteur GGUF r√©el (`fragmenter.py` + `recombiner.py` fonctionnels sur TinyLlama) |
| Phase 3 | ‚è≥ | Inf√©rence distribu√©e r√©elle (matmul multi-processus, v√©rifi√© vs llama.cpp) |
| Phase 4 | ‚è≥ | R√©seau P2P r√©el (libp2p, WebRTC, DHT) |
| Phase 5 | ‚è≥ | Application utilisateur (Tauri desktop, PWA mobile) |
| Phase 6 | ‚è≥ | Passage √† l'√©chelle, incentives, support multi-mod√®les |

**Mod√®le cible : Mistral Large 3 (675B, MoE, ~46 000 fragments √ó 10 Mo)**

---

## Probl√®mes connus

- **Vitesse du moteur Python** : ~14s/token (pas de cache KV, pas de batching, NumPy pur). Utiliser `p2p_bridge.py` en production.
- **Gradio 6.x** : `theme=` et `css=` doivent √™tre pass√©s √† `.launch()`, pas √† `gr.Blocks()`. Pas de `type="messages"` dans `gr.Chatbot()`. Pour ajouter un nouveau backend de distribution, mettre √† jour le dict `DISTRIBUTION_MODES` et le dispatch dans `load_model()` dans `app.py`.
- **Scripts de d√©bogage** (`debug*.py`, `test_fix.py`) √† la racine du projet sont temporaires ‚Äî ne font pas partie du code de production.
